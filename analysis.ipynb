{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trending Youtube Video Analysis \n",
    "#### by Sam Logsdon, Maryam Bokhari, and Jeong Rae Park"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data from the Database \n",
    "This will initialize the database if your directories are setup according to the readme. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import db\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os.path\n",
    "\n",
    "DB_PATH = 'db.sqlite'\n",
    "DATA_PATH = 'data'\n",
    "\n",
    "if not (os.path.exists(DB_PATH)):\n",
    "    import db_loader\n",
    "    skipped = db_loader.run(DATA_PATH, DB_PATH)\n",
    "    \n",
    "\n",
    "db = db.DB(DB_PATH)\n",
    "base_df = db.fetch_videos_as_df() # make a copy of this using base_df.copy()\n",
    "categories = db.fetch_categories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shared Videos Between Countries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in viewing the proportion of shared trending videos between countries. The data for this analysis was calculated using a self-join of the videos table in sqlite on the basis of video_id and trending_date. The country was selected from each side of the join as well, and rows where both countries were the same were excluded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "SQL = '''\n",
    "SELECT v1.country c1, v2.country c2, count(*) videos\n",
    "             FROM video v1\n",
    "                      JOIN video v2\n",
    "                           ON (v2.video_id = v1.video_id) AND (v2.trending_date = v1.trending_date)\n",
    "             WHERE c1 != c2\n",
    "GROUP BY c1, c2\n",
    "'''\n",
    "rows = db.conn.execute(SQL).fetchall()\n",
    "df = pd.DataFrame.from_records(data = rows, columns=['c1', 'c2', 'videos'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_videos_df = base_df.groupby(level=[0]).agg(total_videos=('views', 'count'))\n",
    "total_videos_df.join(df.set_index('c1'), rsuffix='c1').join(df.set_index('c2'), lsuffix='_c1',rsuffix='_c2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_videos_df['key'] = 0\n",
    "vddf = total_videos_df.reset_index().merge(total_videos_df.reset_index(), on='key').drop('key', axis=1).set_index(['country_x', 'country_y'])\n",
    "vddf.index = vddf.index.rename(['c1', 'c2'])\n",
    "df = df.set_index(['c1', 'c2']).join(vddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df['total_videos'] = df.total_videos_x + df.total_videos_y\n",
    "df['shared_percent'] = df.videos / df.total_videos \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivoted Dataframe\n",
    "We use the pandas pivot function to reshape our data. Note that the data is mirrored across the line of missing(nan) values. This is because the self-join query returned data for (c1,c2) and (c2,c1) pairs e.g. (US,RU) and (RU,US). We will clean this up in the next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv = df.reset_index().pivot(index='c1', columns='c2', values='shared_percent')\n",
    "pv.rename_axis(None).style.set_table_styles([{'selector': '.index_name', 'props': [('font-size', '0')]}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generated heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"ticks\", font_scale=1.2)\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(12,8)) \n",
    "\n",
    "\n",
    "mask = np.zeros_like(pv, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "ax = sns.heatmap(pv, cmap = sns.color_palette(\"YlGnBu\", 10), \n",
    "                 annot=True, annot_kws = {'size': 'large'}, fmt='.2%',\n",
    "                 mask = mask,\n",
    "                 cbar = False\n",
    "                )\n",
    "# fix for mpl bug that cuts off top/bottom of seaborn viz\n",
    "b, t = plt.ylim() # discover the values for bottom and top\n",
    "b += 0.5 # Add 0.5 to the bottom\n",
    "t -= 0.5 # Subtract 0.5 from the top\n",
    "plt.ylim(b, t) # update the ylim(bottom, top) values\n",
    "\n",
    "ax.xaxis.get_major_ticks()[-1].set_visible(False)\n",
    "ax.yaxis.get_major_ticks()[0].set_visible(False)\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation=0)\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average proportion of shared videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('c1').aggregate(\n",
    "    average_shared=('shared_percent', 'mean')).sort_values(by='average_shared',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average time for video to go from viewable to trending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = base_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing for this question required us to drop duplicate Video Id's such that we keep the first instance after we sort the data.\n",
    "\n",
    "Sample Video ID (Before Pre-processing): \n",
    "\n",
    "| Video ID   |   Trending Date   | Publish Date |\n",
    "|----------|-------------|------|\n",
    "| 1 |  2018-09-01. | 2018-09-01 |\n",
    "| 1 |  2018-09-02.  |   2018-09-01|\n",
    "| 1 |  2018-09-03.  |   2018-09-01 |\n",
    "\n",
    "\n",
    "Sample Video ID (After Pre-processing): \n",
    "\n",
    "| Video ID   |   Trending Date   | Publish Date |\n",
    "|----------|-------------|------|\n",
    "| 1 |  2018-09-01. | 2018-09-01 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre processing data- Dropping Duplicate Values and handling null values \n",
    "m = m.sort_values(by=['country', 'video_id', 'trending_date'])\n",
    "group = m.groupby(level=[1])\n",
    "m = group.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trending age wascalculated by subtracting the trending time with the publish time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m['trending_date']=m['trending_date'].dt.normalize()\n",
    "m['publish_time']=m['publish_time'].dt.normalize()\n",
    "m['time_to_trending']= m['trending_date'].sub(m['publish_time'])\n",
    "m['time_to_trending'].describe()\n",
    "m['time_to_trending'] = m['time_to_trending'].dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the first most frequent timings\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "\n",
    "counts = m.groupby('time_to_trending').agg(count=('likes', 'count')).sort_values(by='time_to_trending')\n",
    "#y=m['time_to_trending'].sort_values().value_counts()#[:10]\n",
    "counts[:15].plot(kind='bar', title=\"Frequency of Time taken to trend videos\", figsize=(15,8))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a shame we don't have a higher resolution for trending_date to get a proper sense of the distribution. From this view, it looks like it could be normally distributed, but the shape could change significantly if we zoomed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data distribution\n",
    "maxi=m['publish_time'].max()\n",
    "mini=m['publish_time'].min()\n",
    "print(mini, '-', maxi)\n",
    "print(m['trending_date'].count() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking a year with the most number of published videos \n",
    "avg=m.groupby(by=[m.publish_time.dt.year]).count()\n",
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "grouped = m.groupby('trending_date').agg(average_time_to_trend=('time_to_trending', 'mean'), \n",
    "                                         error=('time_to_trending', 'std'), \n",
    "                                         mad=('time_to_trending', 'mad'))\n",
    "grouped.plot(kind='line', y='average_time_to_trend',figsize=(15,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Youtube has clearly made some changes to their trending algorithm to prefer more recent content. Before the change, videos from as early as 2006 could trend on the basis of just a few hundred views, causing a huge variance in age. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What type of video category is most likely to become trending."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in viewing list of categories which is most likely to become trending.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3 = base_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this process, we can count number of vedio in each category in each countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3 = q3.groupby(['country', 'category_id']).agg(\n",
    "    count = ('likes', 'count')\n",
    ")\n",
    "q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this code, we can sort the data by count which is number of video and give a rank based on count.\n",
    "After that, we only pull out the top 5 from each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q3 = q3.sort_values(by=['country', 'count'], ascending=False)\n",
    "temp_df = q3.groupby(['country']).agg(rank = ('count', 'cumcount'))\n",
    "q3 = pd.merge(q3, temp_df, left_index=True, right_index=True)\n",
    "q3 = q3.join(categories.set_index('id'), on=['category_id'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, we join two data, q3 and category. \n",
    "After that, we aggregated all number of videos in categories which are not in top 6, and renamed the category id as \"99\" and ranked it as \"7\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete this block before submitting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"whitegrid\")\n",
    "%matplotlib inline\n",
    "\n",
    "bottom = q3[q3['rank']>6].groupby(level=[0]).agg(count=('count', 'sum'))\n",
    "bottom['category_id'] = 99\n",
    "bottom['category'] = 'Other'\n",
    "bottom['rank'] = 7\n",
    "bottom = bottom.reset_index().set_index(['country', 'category_id'])\n",
    "#q3 = q3[q3['rank']<=6].append(bottom, sort=True)\n",
    "q3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make barplot and swarmplot.\n",
    "With swarmplot, we can see that rank of categories base of number of videos in each countries.\n",
    "As you see, Entertainment category is popular in most countries, but not all counties.\n",
    "There are similarity and difference by countries.\n",
    "With barplot, we cass see that mean of number of videos in each countries.\n",
    "in 5 countries, Entertainment category is outliers which means that it is way over popular than other category there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8)) \n",
    "#ax = sns.barplot(x=df3.index.get_level_values(1), y=\"count\",hue=df3.index.get_level_values(1), data=df3)\n",
    "sns.boxplot(x='country', y='count', data=q3.reset_index(), color='.75')\n",
    "ax = sns.swarmplot(x=\"country\", y=\"count\", data=q3[q3['rank']<5].reset_index(), hue='category',hue_order=q3.sort_values('category').category.drop_duplicates(), size=12)\n",
    "\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Popular videos at same time in all counties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in viewing the list of popular videos trended at same time in all countries. \n",
    "\n",
    "The preprocessing of the data for this analysis was calculated using a self-join of the videos table in sqlite on the basis of video_id and trending_date. By using SQL, we can pull out the videos which are trended at the same time in countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from containers import YoutubeVideo\n",
    "\n",
    "videos = []\n",
    "with db.conn:\n",
    "    for row in db.conn.execute(\"SELECT count(video_id) as c, * FROM video WHERE video_id != '#NAME?' GROUP BY trending_date, video_id ORDER BY c DESC \"):\n",
    "        videos.append(row)\n",
    "\n",
    "df4 = pd.DataFrame.from_records(data=videos, columns=['c'] + list(YoutubeVideo._fields))\n",
    "df4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Dataframe, we get the videos which has the count number over 10. It means that the video which has count number over 10 are trended in all counties at same time because we have only 10 counties data. There are some same videos twice or more being trended at the same time in all counties which is duplication, but we did not delete this duplicate because our porpose is to see the list of vedios which are become trending at the same time in all counties. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered = df4.query('c>=10')\n",
    "df_filtered = df_filtered.sort_values(by=['title'], ascending=False)\n",
    "df_new = df_filtered[['title','channel_title','trending_date']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only pull out the some columns, title and channel_title and trending_date, and make table chart to view it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') #supress font warnings\n",
    "\n",
    "collabel=(\"title\",\"channel_title\",\"trending_date\")\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "fig.patch.set_visible(False)\n",
    "\n",
    "ax.axis('off')\n",
    "ax.axis('tight')\n",
    "\n",
    "table_df=ax.table(cellText=df_new.values, colLabels=collabel, loc='center')\n",
    "table_df.auto_set_font_size(False)\n",
    "table_df.set_fontsize(12)\n",
    "table_df.scale(3.5,2.5)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Categories With Machine Learning\n",
    "We used an SGDClassifier because of the memory requirements of many other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = db.fetch_videos_as_df(exclude=['tags', 'thumbnail_link'])\n",
    "df = df.reset_index().groupby('video_id').first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[['country' ,'title','description']]\n",
    "target = df.category_id\n",
    "X_train, X_test, target_train, target_test = train_test_split(X, target, test_size=0.4, random_state=0, stratify=target)\n",
    "\n",
    "column_trans = ColumnTransformer(\n",
    "     [('country_category', OneHotEncoder(), ['country']),\n",
    "     ('description_t', TfidfVectorizer(), 'description')])\n",
    "svm_model = make_pipeline(column_trans, SGDClassifier(loss=\"hinge\", penalty=\"l2\"))\n",
    "svm_model.fit(X_train, target_train)\n",
    "svm_labels = svm_model.predict(X_test)\n",
    "\n",
    "\n",
    "print(classification_report(target_test, svm_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(TfidfVectorizer(), SGDClassifier(loss=\"hinge\", penalty=\"l2\"))\n",
    "model.fit(X_train['description'], target_train)\n",
    "labels = model.predict(X_test['description'])\n",
    "print(classification_report(target_test, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using description alone seems to be about the same as both description and country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
